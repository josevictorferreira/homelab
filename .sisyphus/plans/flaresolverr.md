# Add FlareSolverr to K8s (kubenix, apps ns)

## TL;DR
> Deploy FlareSolverr `ghcr.io/flaresolverr/flaresolverr:v3.4.6` as a **cluster-internal** HTTP service in namespace `apps` via kubenix Nix module(s). Expose via `Service` on `8191`; optional metrics on `8192` if enabled. Verify via `make check` + `make manifests` + in-cluster curl to `POST /v1`.

**Effort**: Medium
**Parallel Execution**: YES (3 waves + final)
**Critical Path**: Decisions → Nix module(s) → `make manifests` → Flux reconcile → in-cluster API probe

---

## Context

### Original Request
- Add FlareSolverr to cluster in namespace `apps` using current kubenix config.
- Repo: https://github.com/FlareSolverr/FlareSolverr
- Image: `ghcr.io/flaresolverr/flaresolverr:v3.4.6`

### Known App/Repo Conventions (local)
- Add new app modules under `modules/kubenix/apps/`.
- `homelab.kubernetes.namespaces.applications` maps to namespace `apps`.
- Prefer `make check` + `make manifests`.
- Flake evaluates **git tree**; new files must be `git add`’d before Nix/manifest eval.
- Never edit `.k8s/*.yaml` directly (generated by `make manifests`).

### FlareSolverr runtime facts (upstream README)
- API: `POST /v1` on port `8191`.
- Config via env vars; notable: `PORT`, `LOG_LEVEL`, `LOG_HTML`, `PROMETHEUS_ENABLED`, `PROMETHEUS_PORT`.

### Metis Review (guardrails)
- Decide access: internal-only vs ingress.
- Set resource limits (browser-heavy workload).
- Don’t assume `/health` exists; use `tcpSocket` probes unless verified.

---

## Work Objectives

### Core Objective
Add a kubenix-managed FlareSolverr Deployment+Service in `apps` namespace, pinned to v3.4.6, with safe defaults and agent-executable verification.

### Concrete Deliverables
- `modules/kubenix/apps/flaresolverr.nix` (raw K8s resources via kubenix)
- (Optional) `modules/kubenix/apps/flaresolverr-config.enc.nix` (only if proxy/captcha/etc need secrets)
- Generated manifest(s) under `.k8s/apps/` via `make manifests`

### Must NOT Have (guardrails)
- No edits to `.k8s/*.yaml` by hand.
- No plaintext secrets anywhere; only `kubenix.lib.secretsFor` / `*.enc.nix` / SOPS.
- No external exposure by default unless explicitly chosen.

---

## Verification Strategy

### Test Decision
- **Infrastructure exists**: N/A (this repo is Nix infra; verification via `make check` / `make manifests` + kubectl reads)
- **Automated tests**: None; rely on agent-executed QA scenarios.

### Repo Verification Commands
- `git add <new files>` (flake needs staged files)
- `make check`
- `make manifests`

### QA Policy (agent-executed only)
Evidence saved under `.sisyphus/evidence/`.
- Build-time: `make check`, `make manifests` logs captured.
- Runtime: in-cluster curl against service DNS.

### Defaults Applied (override if you disagree)
- Exposure: **internal-only** (ClusterIP Service; no ingress).
- Metrics: **disabled** (`PROMETHEUS_ENABLED` not set).
- TZ: `Etc/UTC`.
- Probes: `tcpSocket` on 8191 (avoids external internet dependency).

---

## Execution Strategy (parallel waves)

Wave 1 (foundation / decisions; mostly parallel):
- T1 Decide exposure + DNS name (internal-only vs ingress)
- T2 Decide runtime knobs (metrics, log level, TZ, concurrency)
- T3 Pick baseline resources + hardening (securityContext)

Wave 2 (implementation; parallel where possible):
- T4 Implement raw Deployment+Service module
- T5 Optional: add secrets/config module if needed
- T6 Wire optional ingress/HTTPRoute if chosen

Wave 3 (verification + rollout):
- T7 Stage files + run `make check` / `make manifests`
- T8 Deploy via GitOps (commit/push + flux reconcile) + runtime probes

---

## TODOs

- [x] 1. Decide exposure + consumers (internal-only vs ingress)

  **What to do**:
  - Confirm which apps will call FlareSolverr (e.g., prowlarr/jackett/radarr custom scripts).
  - Choose exposure model:
    - Default: ClusterIP only, reachable at `http://flaresolverr.apps.svc.cluster.local:8191`.
    - Optional: Ingress using `kubenix.lib.ingressFor "flaresolverr"` (exposes HTTP).

  **Must NOT do**:
  - Don’t expose publicly without explicit decision.

  **Recommended Agent Profile**:
  - **Category**: `quick`
  - **Skills**: none

  **Parallelization**:
  - **Can Run In Parallel**: YES (with Tasks 2-3)
  - **Blocks**: 4, 6

  **References**:
  - `config/kubernetes.nix` — confirms apps namespace mapping.
  - `modules/kubenix/_lib/default.nix:ingressFor, serviceHostFor` — internal DNS + ingress pattern.

  **Acceptance Criteria**:
  - [ ] Decision recorded in plan: `internal-only` or `ingress`.

  **QA Scenarios**:
  - Scenario: confirm chosen consumer(s) + access path
    - Tool: repo grep/read
    - Steps: search for existing FlareSolverr usage; if none, record “new consumer TBD”.
    - Evidence: `.sisyphus/evidence/task-1-consumers.txt`

- [x] 2. Define runtime config defaults (env vars)

  **What to do**:
  - Set sane defaults (non-secret) in the Deployment:
    - `PORT=8191`
    - `LOG_LEVEL=info`
    - `TZ=Etc/UTC` (or your preferred TZ)
    - Optional metrics: `PROMETHEUS_ENABLED=true` + `PROMETHEUS_PORT=8192` (only if you want scraping)
  - Explicitly *do not* configure CAPTCHA solver unless you have a working one.

  **Must NOT do**:
  - Don’t add proxy credentials as plaintext env vars.

  **Recommended Agent Profile**:
  - **Category**: `quick`
  - **Skills**: `writing-nix-code`

  **Parallelization**:
  - **Can Run In Parallel**: YES (with Tasks 1, 3)
  - **Blocks**: 4, 5

  **References**:
  - Upstream README — env vars + ports (`PORT`, `LOG_LEVEL`, `PROMETHEUS_*`).
  - `modules/kubenix/_lib/default.nix:secretsFor` — secret wiring if needed.

  **Acceptance Criteria**:
  - [ ] Plan explicitly lists chosen env vars and whether metrics enabled.

  **QA Scenarios**:
  - Scenario: confirm env vars match plan
    - Tool: repo read
    - Steps: after implementation, check generated manifest includes expected env vars.
    - Evidence: `.sisyphus/evidence/task-2-env.txt`

- [x] 3. Set baseline resources + hardening

  **What to do**:
  - Choose baseline requests/limits suitable for headless Chrome workload.
    - Default suggestion: requests `cpu: 200m`, `memory: 512Mi`; limits `cpu: 2`, `memory: 2Gi`.
  - Add security hardening where compatible:
    - `runAsNonRoot: true`
    - `allowPrivilegeEscalation: false`
    - `readOnlyRootFilesystem: false` (browser often needs writable dirs; confirm image)
    - drop all capabilities
  - Add terminationGracePeriodSeconds (browser shutdown).

  **Recommended Agent Profile**:
  - **Category**: `unspecified-high`
  - **Skills**: none

  **Parallelization**:
  - **Can Run In Parallel**: YES (with Tasks 1-2)
  - **Blocks**: 4

  **References**:
  - Upstream README — indicates browser-heavy process.
  - Existing app manifests under `modules/kubenix/apps/` — copy securityContext patterns if present.

  **Acceptance Criteria**:
  - [ ] Requests/limits defined.
  - [ ] SecurityContext defined (or explicit rationale why not).

  **QA Scenarios**:
  - Scenario: resource limits present in manifest
    - Tool: grep generated YAML
    - Evidence: `.sisyphus/evidence/task-3-resources.txt`

- [x] 4. Implement FlareSolverr Deployment + Service (raw kubenix resources)

  **What to do**:
  - Add `modules/kubenix/apps/flaresolverr.nix` defining:
    - `kubernetes.resources.deployments.flaresolverr` (1 replica)
    - `kubernetes.resources.services.flaresolverr` (ClusterIP; port name `http`, port 8191)
    - Labels/selector: `app = "flaresolverr"`.
  - Configure container:
    - Image `ghcr.io/flaresolverr/flaresolverr:v3.4.6` (pin digest if available; else plan to resolve via `make manifests`/nix).
    - Container port 8191.
  - Probes:
    - Prefer an HTTP GET if upstream has a health endpoint; otherwise use `tcpSocket` probe on 8191 (safe default).
    - Avoid probes that depend on external internet.

  **Must NOT do**:
  - Don’t assume `/health` exists without verifying.

  **Recommended Agent Profile**:
  - **Category**: `quick`
  - **Skills**: `writing-nix-code`

  **Parallelization**:
  - **Can Run In Parallel**: NO (depends on Tasks 1-3)
  - **Blocked By**: 1, 2, 3
  - **Blocks**: 7, 8

  **References**:
  - `modules/kubenix/AGENTS.md` — shows `kubernetes.resources.deployments.<name> = { ... }` pattern.
  - `modules/kubenix/apps/_shared-subfolders-proton-sync.nix` — raw Service definition pattern.
  - `config/kubernetes.nix` — use `homelab.kubernetes.namespaces.applications`.

  **Acceptance Criteria**:
  - [ ] `modules/kubenix/apps/flaresolverr.nix` exists and evaluates.
  - [ ] Service exposes port 8191 with name `http`.

  **QA Scenarios**:
  - Scenario: manifest contains Deployment + Service in `apps` namespace
    - Tool: `make manifests` output + grep `.k8s/apps/`
    - Evidence: `.sisyphus/evidence/task-4-manifest-snippet.txt`

- [x] 5. (Optional) Add secrets/config module for proxy/captcha integrations - SKIPPED (not needed)

  **What to do**:
  - Only if needed: create `modules/kubenix/apps/flaresolverr-config.enc.nix` with Secret keys (proxy creds etc) using `kubenix.lib.secretsFor`.
  - Add keys to `secrets/k8s-secrets.enc.yaml` via `make secrets`.
  - Wire Deployment envFrom secretRef.

  **Must NOT do**:
  - No plaintext secrets.

  **Recommended Agent Profile**:
  - **Category**: `writing-nix-code`
  - **Skills**: `writing-nix-code`

  **Parallelization**:
  - **Can Run In Parallel**: YES (with Task 4 if interface is decided)
  - **Blocks**: 7, 8 (if used)

  **References**:
  - `modules/kubenix/_lib/default.nix:secretsFor` — secret refs.
  - Other `*-config.enc.nix` files in `modules/kubenix/apps/` — key naming conventions.
  - Repo rules: use `sops --set` or `make secrets` workflow.

  **Acceptance Criteria**:
  - [ ] Secret keys exist in encrypted manifest under `.k8s/`.

  **QA Scenarios**:
  - Scenario: secret key injected (vals) and present in generated `.enc.yaml`
    - Tool: `sops -d` on generated secret
    - Evidence: `.sisyphus/evidence/task-5-secret.txt`

- [x] 6. (Optional) Add ingress + auth guardrails - SKIPPED (internal-only)

  **What to do**:
  - If exposure chosen: add ingress using `kubenix.lib.ingressFor "flaresolverr"` OR custom ingress block mirroring other apps.
  - Consider adding basic auth / IP allowlist if your ingress stack supports it.

  **Recommended Agent Profile**:
  - **Category**: `unspecified-high`
  - **Skills**: none

  **Parallelization**:
  - **Can Run In Parallel**: YES (after Task 1)
  - **Blocked By**: 1
  - **Blocks**: 8

  **References**:
  - `modules/kubenix/_lib/default.nix:ingressFor` — default ingress template.
  - `modules/kubenix/apps/uptime-kuma.nix` — example custom ingress values.

  **Acceptance Criteria**:
  - [ ] Ingress host resolves to the service (if enabled).

  **QA Scenarios**:
  - Scenario: curl via ingress host returns a response (if enabled)
    - Tool: curl from within cluster or from allowed network
    - Evidence: `.sisyphus/evidence/task-6-ingress.txt`

- [x] 7. Validate generation (git add + make)

  **What to do**:
  - `git add` any new app module files **before** running:
    - `make check`
    - `make manifests`

  **Recommended Agent Profile**:
  - **Category**: `quick`
  - **Skills**: `writing-nix-code`

  **Parallelization**:
  - **Can Run In Parallel**: NO
  - **Blocked By**: 4 (+5/+6 if used)
  - **Blocks**: 8

  **References**:
  - `.docs/rules.md` — flake uses git tree; unstaged files invisible.
  - Makefile — targets `check`/`manifests`.

  **Acceptance Criteria**:
  - [ ] `make check` PASS.
  - [ ] `make manifests` PASS.
  - [ ] `.k8s/apps/` contains flaresolverr manifest.

  **QA Scenarios**:
  - Scenario: capture make logs
    - Tool: bash
    - Evidence: `.sisyphus/evidence/task-7-make-check.txt`, `.sisyphus/evidence/task-7-make-manifests.txt`

- [ ] 8. GitOps rollout + runtime verification (in-cluster)

  **What to do**:
  - Commit changes (ask user before push).
  - Let Flux reconcile or force reconcile (per repo norms).
  - Verify:
    - Deployment ready in `apps`.
    - Service reachable via cluster DNS.
    - API responds to a minimal POST `/v1`.

  **Recommended Agent Profile**:
  - **Category**: `unspecified-high`
  - **Skills**: `kubernetes-tools`, `git-master`

  **Parallelization**:
  - **Can Run In Parallel**: NO
  - **Blocked By**: 7
  - **Blocks**: Final Verification Wave

  **References**:
  - Upstream README — `/v1` request format.
  - `modules/kubenix/_lib/default.nix:serviceHostFor` — expected DNS name.

  **Acceptance Criteria**:
  - [ ] `kubectl get deploy -n apps flaresolverr` shows ready replicas.
  - [ ] In-cluster curl returns JSON for `cmd=request.get`.

  **QA Scenarios**:
  - Scenario: service connectivity + API response
    - Tool: `kubectl` + ephemeral curl pod
    - Steps:
      1. `kubectl -n apps get pods -l app=flaresolverr -o wide`
      2. `kubectl -n apps run fs-test --image=curlimages/curl --restart=Never -i --rm -- \
            curl -sS -X POST http://flaresolverr.apps.svc.cluster.local:8191/v1 \
            -H 'Content-Type: application/json' \
            -d '{"cmd":"request.get","url":"https://example.com","maxTimeout":60000}'`
      3. Assert HTTP 200 and response JSON contains top-level key `status` and object `solution`.
      4. If `status == "ok"`, assert `solution.response` is non-empty.
    - Evidence: `.sisyphus/evidence/task-8-api.json`

---

## Final Verification Wave

- [ ] F1. Plan Compliance / Scope Audit — `oracle`
- [ ] F2. Nix/K8s Quality Review — `unspecified-high`
- [ ] F3. Runtime QA Replay (all scenarios) — `unspecified-high`
- [ ] F4. Scope Fidelity Check — `deep`

---

## Commit Strategy
- Prefer 1 commit for feature + generated manifests.
- Never push/merge without explicit user consent.

## Success Criteria
- `make check` PASS
- `make manifests` PASS and generates `.k8s/apps/flaresolverr*.yaml`
- FlareSolverr pod Ready in `apps` namespace
- In-cluster request to `POST http://flaresolverr.apps.svc.cluster.local:8191/v1` returns JSON (non-error) for a simple URL fetch
