apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  annotations:
    kubenix/k8s-version: '1.32'
    kubenix/project-name: ze-homelab
  name: ceph-blockpool
  namespace: rook-ceph
spec:
  failureDomain: host
  replicated:
    size: 3
---
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  annotations:
    kubenix/k8s-version: '1.32'
    kubenix/project-name: ze-homelab
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephBlockPools:
    - name: replicapool
      spec:
        failureDomain: host
        replicated:
          size: 3
      storageClass:
        alllowVolumeExpansion: true
        enabled: true
        isDefault: true
        name: rook-ceph-block
        reclaimPolicy: Delete
  cephFileSystems:
    - name: ceph-filesystem
      spec:
        dataPools:
          - replicated:
              size: 3
        metadataPool:
          replicated:
            size: 3
        metadataServer:
          activeCount: 1
      storageClass:
        allowVolumeExpansion: true
        enabled: true
        name: rook-ceph-filesystem
        pool: data0
        reclaimPolicy: Delete
  cephVersion:
    allowUnsupported: false
    image: quay.io/ceph/ceph:v19.2.3
  cleanupPolicy:
    allowUninstallWithVolumes: false
    confirmation: yes-really-destroy-data
    sanitizeDisks:
      dataSource: zero
      iteration: 1
      method: quick
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  crashCollector:
    disable: false
  dashboard:
    enabled: true
    ssl: true
  dataDirHostPath: /var/lib/rook
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    livenessProbe:
      mgr:
        disabled: false
      mon:
        disabled: false
      osd:
        disabled: false
  ingress:
    dashboard:
      annotations:
        cert-manager.io/cluster-issuer: cloudflare-issuer
      enabled: true
      host:
        name: ceph.josevictor.me
        path: /
      ingressClassName: cilium
      tls:
        - hosts:
            - ceph.josevictor.me
          secretName: wildcard-tls
  logCollector:
    enabled: true
    maxLogSize: 500M
    periodicity: daily
  mgr:
    allowMultiplePerNode: false
    count: 2
  mon:
    allowMultiplePerNode: false
    count: 3
  monitoring:
    enabled: false
  network:
    connections:
      compression:
        enabled: false
      encryption:
        enabled: false
      requireMsgr2: false
  priorityClassNames:
    mgr: system-cluster-critical
    mon: system-node-critical
    osd: system-node-critical
  removeOSDsIfOutAndSafeToRemove: false
  resources:
    cleanup:
      limits:
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 100Mi
    crashcollector:
      limits:
        memory: 60Mi
      requests:
        cpu: 100m
        memory: 60Mi
    exporter:
      limits:
        memory: 128Mi
      requests:
        cpu: 50m
        memory: 50Mi
    logcollector:
      limits:
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 100Mi
    mgr:
      limits:
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 512Mi
    mgr-sidecar:
      limits:
        memory: 100Mi
      requests:
        cpu: 100m
        memory: 40Mi
    mon:
      limits:
        memory: 2Gi
      requests:
        cpu: 1000m
        memory: 1Gi
    osd:
      limits:
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 4Gi
    prepareosd:
      requests:
        cpu: 500m
        memory: 50Mi
  skipUpgradeChecks: false
  storage:
    dashboard:
      enabled: true
    nodes:
      - devices:
          - name: /dev/disk/by-partlabel/CEPH_OSD_NVME
          - name: /dev/disk/by-partlabel/CEPH_OSD_SATA
        name: lab-alpha-cp
      - devices:
          - name: /dev/disk/by-partlabel/CEPH_OSD_NVME
        name: lab-beta-cp
      - devices:
          - name: /dev/disk/by-partlabel/CEPH_OSD_NVME
        name: lab-delta-cp
      - devices:
          - name: /dev/disk/by-partlabel/CEPH_OSD_NVME
          - name: /dev/disk/by-partlabel/CEPH_OSD_SATA
        name: lab-gamma-wk
    resources:
      cleanup:
        requests:
          cpu: 50m
          memory: 64Mi
      crashcollector:
        requests:
          cpu: 50m
          memory: 60Mi
      exporter:
        requests:
          cpu: 50m
          memory: 64Mi
      logcollector:
        requests:
          cpu: 50m
          memory: 64Mi
      mgr:
        limits:
          memory: 1Gi
        requests:
          cpu: 50m
          memory: 64Mi
      mgr-sidecar:
        requests:
          cpu: 50m
          memory: 40Mi
      mon:
        limits:
          memory: 2Gi
        requests:
          cpu: 50m
          memory: 64Mi
      osd:
        limits:
          memory: 4Gi
        requests:
          cpu: 50m
          memory: 64Mi
      prepareosd:
        requests:
          cpu: 50m
          memory: 50Mi
    useAllDevices: false
    useAllNodes: false
  upgradeOSDRequiresHealthyPGs: false
  waitTimeoutForHealthyOSDInMinutes: 10
---
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  annotations:
    kubenix/k8s-version: '1.32'
    kubenix/project-name: ze-homelab
  name: ceph-filesystem
  namespace: rook-ceph
spec:
  dataPools:
    - failureDomain: host
      name: data0
      replicated:
        size: 3
  metadataPool:
    replicated:
      size: 3
  metadataServer:
    activeCount: 1
    activeStandby: true
    priorityClassName: system-cluster-critical
    resources:
      limits:
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 4Gi
---
apiVersion: ceph.rook.io/v1
kind: CephFilesystemSubVolumeGroup
metadata:
  annotations:
    kubenix/k8s-version: '1.32'
    kubenix/project-name: ze-homelab
  name: ceph-filesystem-csi
  namespace: rook-ceph
spec:
  filesystemName: ceph-filesystem
  name: csi
  pinning:
    distributed: 1
---
apiVersion: ceph.rook.io/v1
kind: CephObjectStore
metadata:
  annotations:
    kubenix/k8s-version: '1.32'
    kubenix/project-name: ze-homelab
  name: ceph-objectstore
  namespace: rook-ceph
spec:
  dataPool:
    erasureCoded:
      codingChunks: 1
      dataChunks: 2
    failureDomain: host
    parameters:
      bulk: 'true'
  gateway:
    instances: 1
    port: 80
    priorityClassName: system-cluster-critical
    resources:
      limits:
        memory: 2Gi
      requests:
        cpu: 1000m
        memory: 1Gi
  metadataPool:
    failureDomain: host
    replicated:
      size: 3
  preservePoolsOnDelete: true
---
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    kubenix/k8s-version: '1.32'
    kubenix/project-name: ze-homelab
    storageclass.kubernetes.io/is-default-class: 'true'
  name: ceph-block
  namespace: rook-ceph
parameters:
  clusterID: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/fstype: ext4
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  imageFeatures: layering
  imageFormat: '2'
  pool: ceph-blockpool
provisioner: rook-ceph.rbd.csi.ceph.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    kubenix/k8s-version: '1.32'
    kubenix/project-name: ze-homelab
  name: ceph-bucket
  namespace: rook-ceph
parameters:
  objectStoreName: ceph-objectstore
  objectStoreNamespace: rook-ceph
  region: us-east-1
provisioner: rook-ceph.ceph.rook.io/bucket
reclaimPolicy: Delete
volumeBindingMode: Immediate
---
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    kubenix/k8s-version: '1.32'
    kubenix/project-name: ze-homelab
    storageclass.kubernetes.io/is-default-class: 'false'
  name: ceph-filesystem
  namespace: rook-ceph
parameters:
  clusterID: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/fstype: ext4
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  fsName: ceph-filesystem
  pool: ceph-filesystem-data0
provisioner: rook-ceph.cephfs.csi.ceph.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
