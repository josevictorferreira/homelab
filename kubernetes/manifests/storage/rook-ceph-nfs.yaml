apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    kubenix/k8s-version: '1.32'
    kubenix/project-name: ze-homelab
  name: homelab-nfs-ceph-export-task
  namespace: rook-ceph
spec:
  backoffLimit: 3
  template:
    spec:
      containers:
        - command:
            - /bin/bash
            - -lc
            - "set -euo pipefail\nEXPORT_ID=10\nSUBVOL_GROUP='nfs-exports'\nSUBVOL_NAME='homelab-nfs'\n\
              FS='ceph-filesystem'\nCLUSTER='homelab-nfs'\nNFSNS='homelab-nfs'\nRADOS_POOL='.nfs'\n\
              \nCEPH_CONFIG=/etc/ceph/ceph.conf\nMON_CONFIG=/etc/rook/mon-endpoints\n\
              KEYRING_FILE=/etc/ceph/keyring\nendpoints=$(cat \"$MON_CONFIG\")\nmon_endpoints=$(echo\
              \ \"$endpoints\" | sed 's/[a-z0-9_-]\\+=//g')\nmkdir -p /etc/ceph\n\
              cat > \"$CEPH_CONFIG\" <<EOF\n[global]\nmon_host = $mon_endpoints\n\
              [client.admin]\nkeyring = $KEYRING_FILE\nEOF\nif   [ -f /var/lib/rook-ceph-mon/ceph-secret\
              \ ];  then ceph_secret=$(cat /var/lib/rook-ceph-mon/ceph-secret)\nelif\
              \ [ -f /var/lib/rook-ceph-mon/admin-secret ]; then ceph_secret=$(cat\
              \ /var/lib/rook-ceph-mon/admin-secret)\nelse echo \"No ceph admin secret\
              \ found\"; exit 2; fi\nif [ -f /var/lib/rook-ceph-mon/ceph-username\
              \ ]; then username=$(cat /var/lib/rook-ceph-mon/ceph-username); else\
              \ username=client.admin; fi\ncat > \"$KEYRING_FILE\" <<EOF\n[$username]\n\
              key = $ceph_secret\nEOF\n\nceph -c \"$CEPH_CONFIG\" mgr module enable\
              \ nfs || true\nceph -c \"$CEPH_CONFIG\" mgr module enable rook || true\n\
              ceph -c \"$CEPH_CONFIG\" orch set backend rook || true\n\necho \"Updating\
              \ RADOS pool $RADOS_POOL Auth Caps\"\nfor SUFFIX in a; do\n  ID=\"client.nfs-ganesha.$${CLUSTER}.$${SUFFIX}\"\
              \n  echo \"Creating ID $ID\"\n  ceph -c \"$CEPH_CONFIG\" auth get-or-create\
              \ \"$ID\" \\\n    mon 'allow r' \\\n    mgr 'allow rw' \\\n    osd \"\
              allow rw pool=$${RADOS_POOL} namespace=$${NFSNS}\" >/dev/null || true\n\
              done\n\nif ! SUBVOL_PATH=\"$(\n  ceph -c \"$CEPH_CONFIG\" fs subvolume\
              \ getpath \"$FS\" \"$SUBVOL_NAME\" --group_name \"$SUBVOL_GROUP\" 2>/dev/null\n\
              )\"; then\n  echo \"Creating subvolume group and subvolume...\"\n  if\
              \ ! ceph -c \"$CEPH_CONFIG\" fs subvolumegroup ls \"$FS\" -f json 2>/dev/null\
              \ \\\n      | grep -q \"\\\"name\\\"[[:space:]]*:[[:space:]]*\\\"$SUBVOL_GROUP\\\
              \"\"; then\n    echo \"Creating subvolume group $SUBVOL_GROUP in filesystem\
              \ $FS\"\n    ceph -c \"$CEPH_CONFIG\" fs subvolumegroup create \"$FS\"\
              \ \"$SUBVOL_GROUP\"\n  fi\n\n  if ! ceph -c \"$CEPH_CONFIG\" fs subvolume\
              \ info \"$FS\" \"$SUBVOL_NAME\" --group_name \"$SUBVOL_GROUP\" >/dev/null\
              \ 2>&1; then\n    echo \"Creating subvolume $SUBVOL_NAME in group $SUBVOL_GROUP\"\
              \n    ceph -c \"$CEPH_CONFIG\" fs subvolume create \"$FS\" \"$SUBVOL_NAME\"\
              \ \\\n      --group_name \"$SUBVOL_GROUP\" --size 0 --uid 2002 --gid\
              \ 2002 --mode 2775\n  fi\n\n  SUBVOL_PATH=\"$(ceph -c \"$CEPH_CONFIG\"\
              \ fs subvolume getpath \"$FS\" \"$SUBVOL_NAME\" --group_name \"$SUBVOL_GROUP\"\
              )\"\nfi\n\necho \"Subvolume path: $SUBVOL_PATH\"\n\nEXPORT_JSON='{\"\
              access_type\":\"RW\",\"clients\":[{\"access_type\":\"RW\",\"addresses\"\
              :\"*\",\"protocol\":\"4\",\"sectype\":[\"sys\"],\"squash\":\"all_squash\"\
              }],\"export_id\":10,\"fsal\":{\"fs_name\":\"ceph-filesystem\",\"name\"\
              :\"CEPH\"},\"path\":\"__SUBVOL_PATH__\",\"pseudo\":\"/homelab-nfs\"\
              ,\"security_label\":false,\"squash\":\"all_squash\"}'\nEXPORT_JSON=\"\
              $${EXPORT_JSON/__SUBVOL_PATH__/$SUBVOL_PATH}\"\nprintf '%s' \"$EXPORT_JSON\"\
              \ > /tmp/export_final.json\n\nceph -c \"$CEPH_CONFIG\" nfs export apply\
              \ \"$CLUSTER\" -i /tmp/export_final.json\n\nrados -p .nfs --namespace\
              \ $NFSNS get \"conf-nfs.$CLUSTER\"     /tmp/conf-nfs               \
              \ || true\nrados -p .nfs --namespace $NFSNS get \"export-$EXPORT_ID\"\
              \     /tmp/export-$$EXPORT_ID     || true\n\necho \"Fetching user_id\
              \ from export-$EXPORT_ID\"\n\necho \"--------------------------- CONTENTS\
              \ -----------------------------\"\ncat /tmp/conf-nfs               \
              \ || echo \"(conf-nfs not found)\"\necho \"------------------------------------------------------------------\"\
              \ncat \"/tmp/export-$EXPORT_ID\"     || echo \"(export-$CLUSTER not\
              \ found)\"\necho \"------------------------------------------------------------------\"\
              \ncat /tmp/export_base.conf        || echo \"(export_base.conf not found)\"\
              \n\necho \"Restarting NFS Ganesha grace...\"\nfor SUFFIX in a; do\n\
              \  ganesha-rados-grace --pool .nfs --ns \"$NFSNS\" add \"$${CLUSTER}.$${SUFFIX}\"\
              \   || true\n  ganesha-rados-grace --pool .nfs --ns \"$NFSNS\" start\
              \ \"$${CLUSTER}.$${SUFFIX}\" || true\ndone\n\necho \"Removing orchestrator\
              \ backend...\"\nceph -c \"$CEPH_CONFIG\" orch set backend \"\" || true\n\
              \necho \"DONE.\"\n"
          image: quay.io/ceph/ceph:v19
          name: apply
          volumeMounts:
            - mountPath: /etc/rook
              name: mon-endpoints
            - mountPath: /etc/ceph
              name: ceph-config
            - mountPath: /var/lib/rook-ceph-mon
              name: ceph-admin-secret
              readOnly: true
      restartPolicy: OnFailure
      serviceAccountName: rook-ceph-default
      volumes:
        - configMap:
            items:
              - key: data
                path: mon-endpoints
            name: rook-ceph-mon-endpoints
          name: mon-endpoints
        - emptyDir: {}
          name: ceph-config
        - name: ceph-admin-secret
          secret:
            secretName: rook-ceph-mon
  ttlSecondsAfterFinished: 3600
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    kubenix/k8s-version: '1.32'
    kubenix/project-name: ze-homelab
  name: homelab-nfs-ganesha-conf-patch
  namespace: rook-ceph
spec:
  backoffLimit: 2
  template:
    spec:
      containers:
        - args:
            - "set -euo pipefail\nNS='rook-ceph'\nCLUSTER='homelab-nfs'\nCM=\"\"\n\
              \nexport PATH=/opt/bitnami/kubectl/bin:$PATH\n\necho \"Starting patch\
              \ routine for node ID a...\"\nCM=\"\"\n\nfor i in {1..10}; do\n  if\
              \ [ -n \"$CM\" ]; then break; fi\n  echo \"Waiting for rook-ceph-nfs\
              \ ConfigMap...\"\n  sleep 6\n  CM=\"$(kubectl -n rook-ceph get cm -l\
              \ app=rook-ceph-nfs,instance=a -o name | grep -i rook-ceph | head -n1\
              \ || true)\"\ndone\n\n[ -z \"$CM\" ] && { echo \"ERROR: rook-ceph ganesha\
              \ ConfigMap not found\"; exit 1; }\n\necho \"Patching $CM in rook-ceph...\"\
              \n\nkubectl -n rook-ceph patch \"$CM\" --type merge -p '{\"data\":{\"\
              config\":\"NFS_CORE_PARAM {\\n  Enable_NLM = false;\\n  Enable_RQUOTA\
              \ = false;\\n  Protocols = 4;\\n  Bind_addr = 0.0.0.0;\\n  NFS_Port\
              \ = 2049;\\n  Allow_Set_Io_Flusher_Fail = true;\\n  Enable_malloc_trim\
              \ = true;\\n}\\n\\nMDCACHE {\\n  Dir_Chunk = 0;\\n  Cache_FDs = true;\\\
              n}\\n\\nNFS_KRB5 { Active_krb5 = false; }\\n\\nNFSv4 {\\n  Graceless\
              \ = false;\\n  Delegations = false;\\n  Minor_Versions = 0, 1, 2;\\\
              n  Allow_Numeric_Owners = true;\\n  Only_Numeric_Owners = true;\\n \
              \ RecoveryBackend = \\\"fs_ng\\\";\\n}\\n\\nEXPORT_DEFAULTS {\\n  Attr_Expiration_Time\
              \ = 0;\\n  Protocols = 4;\\n  Transports = TCP;\\n  Access_Type = RW;\\\
              n  Squash = All_Squash;\\n  Manage_Gids = true;\\n  Anonymous_uid =\
              \ 2002;\\n  Anonymous_gid = 2002;\\n  SecType = sys;\\n}\\n\\nRADOS_KV\
              \ {\\n  ceph_conf = \\\"/etc/ceph/ceph.conf\\\";\\n  userid = nfs-ganesha.homelab-nfs.a;\\\
              n  nodeid = homelab-nfs.a;\\n  pool = \\\".nfs\\\";\\n  namespace =\
              \ \\\"homelab-nfs\\\";\\n}\\n\\nCEPH { Ceph_Conf = \\\"/etc/ceph/ceph.conf\\\
              \"; }\\n\\nRADOS_URLS {\\n  ceph_conf = \\\"/etc/ceph/ceph.conf\\\"\
              ;\\n  userid = nfs-ganesha.homelab-nfs.a;\\n  watch_url = \\\"rados://.nfs/homelab-nfs/conf-nfs.homelab-nfs\\\
              \";\\n}\\n\\n%url    \\\"rados://.nfs/homelab-nfs/conf-nfs.homelab-nfs\\\
              \"\\n\"}}'\n\n\n\necho \"Restarting deployment for node ID a...\"\n\
              DEP=\"$(kubectl -n rook-ceph get deploy -l app=rook-ceph-nfs,instance=a\
              \ -o name | grep -i rook-ceph | head -n1 || true)\"\n[ -z \"$DEP\" ]\
              \ && { echo \"WARN: deployment not found; skipping restart\"; exit 0;\
              \ }\nkubectl -n rook-ceph rollout restart \"$DEP\"\nkubectl -n rook-ceph\
              \ rollout status  \"$DEP\"\n\n\n\necho \"Done.\"\n"
          command:
            - /bin/bash
            - -lc
          image: bitnami/kubectl:1.30
          name: patch
      restartPolicy: OnFailure
      serviceAccountName: rook-ceph-default
  ttlSecondsAfterFinished: 3600
---
apiVersion: ceph.rook.io/v1
kind: CephNFS
metadata:
  annotations:
    kubenix/k8s-version: '1.32'
    kubenix/project-name: ze-homelab
  name: homelab-nfs
  namespace: rook-ceph
spec:
  server:
    active: 1
    logLevel: NIV_WARN
    placement:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
    resources:
      limits:
        memory: 1Gi
      requests:
        cpu: 50m
        memory: 64Mi
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kubenix/k8s-version: '1.32'
    kubenix/project-name: ze-homelab
    lbipam.cilium.io/ips: 10.10.10.150
    lbipam.cilium.io/sharing-key: nfs
  name: homelab-nfs
  namespace: rook-ceph
spec:
  externalTrafficPolicy: Cluster
  ports:
    - name: nfs-tcp
      nodePort: 30325
      port: 2049
      protocol: TCP
      targetPort: 2049
    - name: nfs-udp
      nodePort: 30326
      port: 2049
      protocol: UDP
      targetPort: 2049
  selector:
    app: rook-ceph-nfs
    ceph_daemon_type: nfs
  type: LoadBalancer
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  annotations:
    kubenix/k8s-version: '1.32'
    kubenix/project-name: ze-homelab
  name: homelab-nfs-ganesha-conf-patch-role
  namespace: rook-ceph
rules:
  - apiGroups:
      - ''
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - patch
  - apiGroups:
      - apps
    resources:
      - deployments
    verbs:
      - get
      - list
      - watch
      - update
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  annotations:
    kubenix/k8s-version: '1.32'
    kubenix/project-name: ze-homelab
  name: homelab-nfs-ganesha-conf-patch-rb
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: homelab-nfs-ganesha-conf-patch-role
subjects:
  - kind: ServiceAccount
    name: rook-ceph-default
    namespace: rook-ceph
