# HOMELAB KNOWLEDGE BASE

**Generated:** 2026-01-28
**Commit:** 2935f27
**Branch:** main

> **Before starting any implementation, read `.docs/rules.md` for project-specific lessons and gotchas.**

## OVERVIEW

NixOS/k3s hybrid homelab: 4 x86 nodes + 1 Pi. Immutable NixOS hosts, kubenix-generated K8s manifests, Flux GitOps.

## STRUCTURE

```
homelab/
├── config/           # Cluster config (nodes.nix, kubernetes.nix, paths.nix)
├── hosts/            # Host entry + hardware/ machine configs
├── modules/
│   ├── profiles/     # Role-based NixOS modules (k8s-*, amd-gpu, etc.)
│   ├── common/       # Shared NixOS settings
│   ├── services/     # Custom NixOS services (haproxy, keepalived)
│   └── kubenix/      # K8s manifest generation (see kubenix/AGENTS.md)
├── secrets/          # SOPS-encrypted secrets (.enc.yaml)
├── .k8s/             # Generated manifests (DO NOT EDIT - regenerated by make)
└── Makefile          # Primary CLI interface
```

## CRITICAL: DATA LOSS PREVENTION

### NEVER Do These (learned from 2026-01-27 incident)

1. **NEVER remove finalizers from Ceph CRDs** - Removing finalizers DESTROYS ALL DATA
   - CephCluster, CephFilesystem, CephBlockPool, CephObjectStore
   - ALWAYS ask user before ANY Ceph CR modification

2. **NEVER delete**: CephCluster, CephFilesystem, CephBlockPool, rook-ceph namespace

3. **NEVER assume Flux recreation is safe** - Ceph CR deletion = data loss

### Before ANY Ceph Operation

```bash
# 1. Check data first - if pools have data, STOP and confirm
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph df

# 2. Check snapshots
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- bash -c 'for img in $(rbd ls replicapool); do rbd snap ls replicapool/$img; done'

# 3. Verify affected PVs
kubectl get pv -o custom-columns='NAME:.metadata.name,STORAGE-CLASS:.spec.storageClassName,CLAIM:.spec.claimRef.name'
```

## WHERE TO LOOK

| Task | Location | Notes |
|------|----------|-------|
| Add new K8s app | `modules/kubenix/apps/` | See apps/AGENTS.md |
| Change node roles | `config/nodes.nix` | Roles auto-enable profiles |
| Add NixOS role | `modules/profiles/` | See profiles/AGENTS.md |
| Hardware config | `hosts/hardware/{machine}.nix` | Per-machine type |
| Cluster secrets | `secrets/k8s-secrets.enc.yaml` | Edit via `make secrets` |
| Storage config | `modules/kubenix/storage/` | Rook-Ceph operator+cluster |
| Cilium/CNI | `modules/kubenix/system/cilium.nix` | Also handles ingress |
| Tailscale config | `modules/profiles/tailscale.nix` | Host-level VPN, subnet routing |

## WORKFLOW

### K8s Manifest Pipeline (ALWAYS use this)

```bash
make manifests  # Full pipeline: generate → inject secrets → encrypt → lock
```

Pipeline stages:
1. `gmanifests` - Nix → YAML via kubenix
2. `vmanifests` - vals injects secrets (ref+sops://...)
3. `umanifests` - Restore unchanged encrypted files from git
4. `emanifests` - SOPS encrypt .enc.yaml files

### NixOS Deployment

```bash
make check     # Validate flake
make deploy    # Interactive single-host deploy (fzf)
make gdeploy   # Deploy by role group (k8s-control-plane, k8s-worker, etc.)
```

### Other Commands

```bash
make kubesync   # Copy kubeconfig from cluster
make reconcile  # Force Flux sync
make secrets    # Edit encrypted secrets (fzf selection)
make lint       # Check nix formatting
make format     # Fix nix formatting
```

## CONVENTIONS

### Naming

- **Hosts**: `lab-{greek}-{suffix}` (alpha-cp, beta-cp, gamma-wk, delta-cp, pi-bk)
- **Apps**: `{app}.nix` + `{app}-config.enc.nix` for secrets
- **Disabled files**: Prefix with `_` (e.g., `_docling.nix`)

### Secrets Flow

1. Secrets defined in `secrets/k8s-secrets.enc.yaml`
2. Referenced in kubenix via `kubenix.lib.secretsFor "key"`
3. Injected by vals during `make vmanifests`
4. Encrypted by SOPS during `make emanifests`

### Storage Classes

- `rook-ceph-block` (default) - RBD block storage
- `rook-ceph-objectstore` - S3-compatible buckets
- CephFS static PVs for shared storage

## ANTI-PATTERNS

| Forbidden | Why |
|-----------|-----|
| Edit `.k8s/*.yaml` directly | Overwritten by `make manifests` |
| `kubectl apply` for persistent changes | Not GitOps; use kubenix |
| Remove Ceph finalizers | DATA LOSS |
| Commit unencrypted secrets | Must be `.enc.yaml` |
| `make gmanifests` alone | Use `make manifests` for full pipeline |

## NODES

| Node | IP | Roles |
|------|----|-------|
| lab-alpha-cp | 10.10.10.200 | control-plane, storage, tailscale, tailscale-router |
| lab-beta-cp | 10.10.10.201 | control-plane, storage, tailscale, tailscale-router |
| lab-gamma-wk | 10.10.10.202 | worker, storage, tailscale |
| lab-delta-cp | 10.10.10.203 | control-plane, storage, amd-gpu, tailscale |
| lab-pi-bk | 10.10.10.209 | backup-server, tailscale |

**VIP**: 10.10.10.250 (HAProxy+Keepalived)

## NETWORK ACCESS

### Tailscale VPN
All nodes run Tailscale for secure remote access:
- **Subnet Routers**: lab-alpha-cp, lab-beta-cp advertise 10.10.10.0/24
- **DNS**: Blocky (10.10.10.100) configured as tailnet nameserver
- **Use Case**: Access LAN devices (k8s VIP, NAS, etc.) from remote locations
- **Auth**: Key stored in hosts-secrets.enc.yaml

### Post-Deploy Steps
After initial Tailscale deployment:
1. Approve routes in Tailscale admin console (Machines → Edit route settings)
2. Enable 10.10.10.0/24 for both subnet routers
3. Connect clients with: `tailscale up --accept-routes`

## INCIDENT RECORD

### 2026-01-27: CephFS Data Loss

Finalizer removed from stuck CephCluster → Flux recreated with empty pools → All CephFS data lost.

**Lesson**: Ceph finalizers PROTECT data. Never remove without explicit confirmation.
