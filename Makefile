.PHONY: check ddeploy deploy gdeploy secrets gmanifests kubesync help 

.DEFAULT_GOAL := help

AVAILABLE_NODE_GROUPS = $(shell nix eval --raw .#listNodeGroups --read-only --quiet)
AVAILABLE_NODES = $(shell nix eval --raw .#listNodes --read-only --quiet)
CONTROL_PLANE_IP = 10.10.10.200
CLUSTER_IP = 10.10.10.250
PORT = 6443
USERNAME = josevictor
REMOTE_KUBECONFIG = /etc/rancher/k3s/k3s.yaml
LOCAL_KUBECONFIG = $(HOME)/.kube/config
CLUSTER_NAME = ze-homelab

check: ## Check if the flake is valid.
	@bash -c "nix flake check --show-trace --all-systems --impure"

ddeploy: ## Dry deploy host.
	@set -e; \
	SEL="$$(printf '%s\n' $(AVAILABLE_NODES) \
    | tr -d '\r' \
	  | fzf --prompt='host> ' --height=40% --border \
	    --preview 'printf \"%s\n\" {}')"; \
  echo "Deploying host: $$SEL"; \
	nix run github:serokell/deploy-rs -- \
    --debug-logs \
		--dry-activate \
		.#$$SEL \
    -- \
    --impure \
    --show-trace

deploy: ## Deploy host.
	@set -e; \
	SEL="$$(printf '%s\n' $(AVAILABLE_NODES) \
    | tr -d '\r' \
	  | fzf --prompt='host> ' --height=40% --border \
	    --preview 'printf \"%s\n\" {}')"; \
  echo "Deploying host: $$SEL"; \
	nix run github:serokell/deploy-rs -- \
    --debug-logs \
		--auto-rollback true \
		.#$$SEL \
    -- \
    --impure \
    --show-trace

gdeploy: ## Deploy hosts that belong to a group.
	@set -e; \
	SEL="$$(printf '%s\n' $(AVAILABLE_NODE_GROUPS) \
    | tr -d '\r' \
	  | fzf --prompt='host> ' --height=40% --border \
	    --preview 'printf \"%s\n\" {}')"; \
  echo "Deploying group: $$SEL"; \
	nix run github:serokell/deploy-rs -- \
    --targets "$$(nix eval --raw .#deployGroups.$$(SEL))" \
    --auto-rollback true

secrets: ## Edit the secrets files.
	@set -e; \
	SEL="$$(find ./secrets -type f -print0 \
	  | fzf --prompt='secret> ' --height=40% --border \
	    --preview 'command -v bat >/dev/null 2>&1 && bat --style=plain --color=always {} || head -n 200 {}')"; \
	test -n "$$SEL" || { echo "No file selected."; exit 1; }; \
	echo "Opening with sops: $$SEL"; \
	sops "$$SEL"


gmanifests: ## Render k8s manifests generated by kubenix.
	@nix build .#gen-manifests --impure --show-trace
	@find kubernetes/manifests -mindepth 1 -maxdepth 1 -type d \
	! \( -name 'flux-system' -o -name 'secrets' \) -exec rm -rf {} +
	@cp -rf result/* kubernetes/manifests
	@rm -rf result
	@find kubernetes/manifests -type f -exec chmod 0644 {} + 
	@find kubernetes/manifests -type d -exec chmod 0755 {} +

kubesync: ## Write kubeconfig from the cluster to kubectl config.
	@set -euo pipefail; \
  kubectl config delete-user "$(USERNAME)" >/dev/null 2>&1 || true; \
	kubectl config delete-cluster "$(CLUSTER_NAME)" >/dev/null 2>&1 || true; \
	kubectl config delete-context "$(CLUSTER_NAME)" >/dev/null 2>&1 || true; \
	tmpdir="$$(mktemp -d)"; \
	tmpkc="$$tmpdir/k3s.yaml"; \
	ssh -4 $(USERNAME)@$(CONTROL_PLANE_IP) "sudo cat $(REMOTE_KUBECONFIG)" > "$$tmpkc"; \
	oldctx="$$(KUBECONFIG="$$tmpkc" kubectl config current-context)"; \
	oldcluster="$$(KUBECONFIG="$$tmpkc" kubectl config view --raw=true -o jsonpath='{.contexts[?(@.name=="'$$oldctx'")].context.cluster}')"; \
	olduser="$$(KUBECONFIG="$$tmpkc" kubectl config view --raw=true -o jsonpath='{.contexts[?(@.name=="'$$oldctx'")].context.user}')"; \
	ca_b64="$$(KUBECONFIG="$$tmpkc" kubectl config view --raw=true -o jsonpath='{.clusters[?(@.name=="'$$oldcluster'")].cluster.certificate-authority-data}')"; \
	clientcrt_b64="$$(KUBECONFIG="$$tmpkc" kubectl config view --raw=true -o jsonpath='{.users[?(@.name=="'$$olduser'")].user.client-certificate-data}')"; \
	clientkey_b64="$$(KUBECONFIG="$$tmpkc" kubectl config view --raw=true -o jsonpath='{.users[?(@.name=="'$$olduser'")].user.client-key-data}')"; \
  echo "$$ca_b64" | base64 -d >"$$tmpdir/ca.crt"; \
  echo "$$clientcrt_b64" | base64 -d >"$$tmpdir/client.crt"; \
  echo "$$clientkey_b64" | base64 -d >"$$tmpdir/client.key"; \
	mkdir -p "$$(dirname "$(LOCAL_KUBECONFIG)")"; \
	[ -f "$(LOCAL_KUBECONFIG)" ] && cp "$(LOCAL_KUBECONFIG)" "$(LOCAL_KUBECONFIG).bak" || true; \
	KUBECONFIG="$(LOCAL_KUBECONFIG)" kubectl config set-cluster "$(CLUSTER_NAME)" --embed-certs=true --server="https://$(CLUSTER_IP):$(PORT)" --certificate-authority="$$tmpdir/ca.crt"; \
  KUBECONFIG="$(LOCAL_KUBECONFIG)" kubectl config set-credentials "$(USERNAME)" --embed-certs=true --client-certificate="$$tmpdir/client.crt" --client-key="$$tmpdir/client.key"; \
	KUBECONFIG="$(LOCAL_KUBECONFIG)" kubectl config set-context "$(CLUSTER_NAME)" --cluster="$(CLUSTER_NAME)" --user="$(USERNAME)"; \
	KUBECONFIG="$(LOCAL_KUBECONFIG)" kubectl config use-context "$(CLUSTER_NAME)" >/dev/null; \
	chmod 600 "$(LOCAL_KUBECONFIG)"; \
	rm -rf "$$tmpdir"; \
	echo "OK: cluster/user/context written â†’ $(LOCAL_KUBECONFIG)"; \

help: ## Show this help.
	@printf "Usage: make [target]\n\nTARGETS:\n"; grep -F "##" $(MAKEFILE_LIST) | grep -Fv "grep -F" | grep -Fv "printf " | sed -e 's/\\$$//' | sed -e 's/##//' | column -t -s ":" | sed -e 's/^/    /'; printf "\n"
